{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab376470",
   "metadata": {},
   "source": [
    "# CCTV Timestamp Digit Classification Pipeline\n",
    "\n",
    "This notebook walks through the complete pipeline to:\n",
    "1. Extract frames from a CCTV video.\n",
    "2. Crop timestamp digits from each frame.\n",
    "3. Manually label the cropped digits.\n",
    "4. Train a digit classification model (using CNN).\n",
    "5. Use the trained model to extract time (HH:MM:SS) from new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6714265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract frames from video\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "video_path = 'cctv_video.mp4'  # Path to your video file\n",
    "output_folder = 'frames'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = 1  # Save one frame per second\n",
    "count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if count % int(cap.get(cv2.CAP_PROP_FPS)) == 0:\n",
    "        frame_id = count // int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        cv2.imwrite(f\"{output_folder}/frame_{frame_id:05d}.png\", frame)\n",
    "    count += 1\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b14cf",
   "metadata": {},
   "source": [
    "## Step 2: Crop individual digits from timestamp area\n",
    "Adjust the crop coordinates based on your timestamp location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968d00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "digit_output = 'digits'\n",
    "os.makedirs(digit_output, exist_ok=True)\n",
    "\n",
    "# Modify these based on your timestamp layout (6 digits: HHMMSS)\n",
    "digit_coords = [\n",
    "    (268, 0, 289, 33),  # hour tens\n",
    "    (289, 0, 310, 33),  # hour units\n",
    "    (338, 0, 359, 33),  # minute tens\n",
    "    (359, 0, 380, 33),  # minute units\n",
    "    (410, 0, 431, 33),  # second tens\n",
    "    (431, 0, 452, 33),  # second units\n",
    "]\n",
    "\n",
    "frame_files = sorted(os.listdir(output_folder))\n",
    "for frame_file in frame_files:\n",
    "    img = cv2.imread(os.path.join(output_folder, frame_file))\n",
    "    for idx, (x1, y1, x2, y2) in enumerate(digit_coords):\n",
    "        digit_crop = img[y1:y2, x1:x2]\n",
    "        digit_path = os.path.join(digit_output, f\"{frame_file[:-4]}_digit{idx}.png\")\n",
    "        cv2.imwrite(digit_path, digit_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2e7bc",
   "metadata": {},
   "source": [
    "## Step 3: Manually Label the Cropped Digits\n",
    "Use a file explorer or labeling tool (like [LabelImg](https://github.com/tzutalin/labelImg)) to label each digit image manually (0-9).\n",
    "\n",
    "Store them in folders:\n",
    "`labeled_digits/0`, `labeled_digits/1`, ..., `labeled_digits/9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f14fb",
   "metadata": {},
   "source": [
    "## Step 4: Train a CNN Model for Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7e99fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4938 images belonging to 10 classes.\n",
      "Found 1229 images belonging to 10 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6668 - loss: 1.0876 - val_accuracy: 0.9561 - val_loss: 0.1853\n",
      "Epoch 2/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9822 - loss: 0.0580 - val_accuracy: 1.0000 - val_loss: 0.0229\n",
      "Epoch 3/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9897 - loss: 0.0242 - val_accuracy: 1.0000 - val_loss: 0.0131\n",
      "Epoch 4/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9845 - loss: 0.0540 - val_accuracy: 0.9780 - val_loss: 0.0420\n",
      "Epoch 5/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9921 - loss: 0.0230 - val_accuracy: 0.9992 - val_loss: 0.0141\n",
      "Epoch 6/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9878 - loss: 0.0281 - val_accuracy: 0.9837 - val_loss: 0.0307\n",
      "Epoch 7/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0146 - val_accuracy: 0.9992 - val_loss: 0.0122\n",
      "Epoch 8/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9919 - loss: 0.0199 - val_accuracy: 0.9919 - val_loss: 0.0182\n",
      "Epoch 9/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9944 - loss: 0.0145 - val_accuracy: 0.9731 - val_loss: 0.0713\n",
      "Epoch 10/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9933 - loss: 0.0178 - val_accuracy: 0.9569 - val_loss: 0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    'manually_labeled_digits',\n",
    "    target_size=(50, 30),  # Adjust to match digit size\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    'manually_labeled_digits',\n",
    "    target_size=(50, 30),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(50, 30, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "model.save('digit_classifier_custom.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dc59d",
   "metadata": {},
   "source": [
    "## Step 5: Predict Timestamp from New Image Using Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6d356a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'labeled_digits/0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m digit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     15\u001b[0m     digit_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(source_dir, \u001b[38;5;28mstr\u001b[39m(digit))\n\u001b[0;32m---> 16\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdigit_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(images)\n\u001b[1;32m     19\u001b[0m     test_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m*\u001b[39m test_ratio)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labeled_digits/0'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "source_dir = 'labeled_digits'\n",
    "test_dir = 'test_digits'\n",
    "test_ratio = 0.2  # 20% of images will go to test set\n",
    "\n",
    "# Create test_digits folders\n",
    "for i in range(10):\n",
    "    os.makedirs(os.path.join(test_dir, str(i)), exist_ok=True)\n",
    "\n",
    "# For each digit class\n",
    "for digit in range(10):\n",
    "    digit_path = os.path.join(source_dir, str(digit))\n",
    "    images = os.listdir(digit_path)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    test_count = int(len(images) * test_ratio)\n",
    "    test_images = images[:test_count]\n",
    "    \n",
    "    for img in test_images:\n",
    "        src = os.path.join(digit_path, img)\n",
    "        dest = os.path.join(test_dir, str(digit), img)\n",
    "        shutil.copy(src, dest)  # use .copy() if you want to keep originals\n",
    "\n",
    "print(\"✅ Dataset split completed! Test images are now in 'test_digits/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9b6490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        24\n",
      "           4       1.00      1.00      1.00        22\n",
      "           5       1.00      1.00      1.00        29\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       261\n",
      "   macro avg       1.00      1.00      1.00       261\n",
      "weighted avg       1.00      1.00      1.00       261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('digit_classifier_custom.h5')\n",
    "\n",
    "# Set path to test dataset (structured as test_digits/0, test_digits/1, ..., test_digits/9)\n",
    "test_dir = 'test_digits'  # Change if your folder name differs\n",
    "img_width, img_height = 30, 50  # Use your model’s input size\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Load images and labels\n",
    "for label in range(10):\n",
    "    folder = os.path.join(test_dir, str(label))\n",
    "    for fname in os.listdir(folder):\n",
    "        path = os.path.join(folder, fname)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=-1)  # Add channel dim\n",
    "        X_test.append(img)\n",
    "        y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Accuracy and detailed report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224c6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Predicted Time: 19:35:15\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load the trained digit classification model\n",
    "model = load_model('digit_classifier_custom.h5')\n",
    "\n",
    "# Define cropping coordinates for each of the 6 digits (x1, y1, x2, y2)\n",
    "# Adjust these based on your actual timestamp format\n",
    "digit_coords = [\n",
    "    (268, 0, 289, 33),  # hour tens\n",
    "    (289, 0, 310, 33),  # hour units\n",
    "    (338, 0, 359, 33),  # minute tens\n",
    "    (359, 0, 380, 33),  # minute units\n",
    "    (410, 0, 431, 33),  # second tens\n",
    "    (431, 0, 452, 33),  # second units\n",
    "]\n",
    "\n",
    "def predict_digit(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (30, 50))  # match model input size\n",
    "    gray = gray.astype('float32') / 255.0\n",
    "    gray = np.expand_dims(gray, axis=(0, -1))\n",
    "    pred = model.predict(gray)\n",
    "    return str(np.argmax(pred))\n",
    "\n",
    "# Read the timestamp image (replace with your filename)\n",
    "timestamp_img = cv2.imread('frames/frame_00508.png')\n",
    "\n",
    "# Crop individual digits\n",
    "digit_images = [timestamp_img[y1:y2, x1:x2] for (x1, y1, x2, y2) in digit_coords]\n",
    "\n",
    "# Predict each digit\n",
    "predicted_digits = [predict_digit(d) for d in digit_images]\n",
    "\n",
    "# Format as HH:MM:SS\n",
    "predicted_time = f\"{predicted_digits[0]}{predicted_digits[1]}:{predicted_digits[2]}{predicted_digits[3]}:{predicted_digits[4]}{predicted_digits[5]}\"\n",
    "print(\"Predicted Time:\", predicted_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d3b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
